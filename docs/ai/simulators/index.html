<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ai.simulators API documentation</title>
<meta name="description" content="Module containing the abstract definition of a simulator, as well as several
implementations of it." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ai.simulators</code></h1>
</header>
<section id="section-intro">
<p>Module containing the abstract definition of a simulator, as well as several
implementations of it.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Module containing the abstract definition of a simulator, as well as several
implementations of it.&#34;&#34;&#34;


from ._tictactoe import TicTacToe
from ._base import Base
from ._connect_four import ConnectFour
from ._factory import Factory
from . import action_spaces


__all__ = [&#34;TicTacToe&#34;, &#34;Base&#34;, &#34;ConnectFour&#34;, &#34;action_spaces&#34;, &#34;Factory&#34;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ai.simulators.action_spaces" href="action_spaces/index.html">ai.simulators.action_spaces</a></code></dt>
<dd>
<div class="desc"><p>Action spaces for simulators.</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ai.simulators.Base"><code class="flex name class">
<span>class <span class="ident">Base</span></span>
<span>(</span><span>deterministic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Simulator base class.</p>
<p>A simulator, as opposed to an environment, executes actions based on a
given state, rather than a interally tracked state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>deterministic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag indicating if this simulator instance is
considered deterministic or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Base(ABC):
    &#34;&#34;&#34;Simulator base class.

    A simulator, as opposed to an environment, executes actions based on a
    given state, rather than a interally tracked state.&#34;&#34;&#34;

    def __init__(self, deterministic: bool) -&gt; None:
        &#34;&#34;&#34;
        Args:
            deterministic (bool): Flag indicating if this simulator instance is
                considered deterministic or not.
        &#34;&#34;&#34;
        super().__init__()
        self._deterministic = deterministic

    @property
    def deterministic(self) -&gt; bool:
        return self._deterministic

    def step(
        self, state: np.ndarray, action: int
    ) -&gt; Tuple[np.ndarray, float, bool, Dict]:
        &#34;&#34;&#34;Executes one step in the environment.

        Args:
            state (np.ndarray): State
            action (int): Action index

        Returns:
            Tuple[np.ndarray, float, bool, Dict]: Tuple of next state, reward, terminal
            flag, and debugging dictionary.
        &#34;&#34;&#34;
        next_states, rewards, terminals, infos = self.step_bulk(
            np.expand_dims(state, 0), np.array([action])
        )
        return next_states[0], rewards[0], terminals[0], infos[0]

    @property
    @abstractmethod
    def action_space(self) -&gt; action_spaces.Base:
        &#34;&#34;&#34;The action space class used by this simulator.&#34;&#34;&#34;
        raise NotImplementedError

    @abstractmethod
    def step_bulk(
        self, states: np.ndarray, actions: np.ndarray
    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:
        &#34;&#34;&#34;Executes a bulk of actions in multiple states.

        Args:
            states (np.ndarray): States, in batch format.
            actions (np.ndarray): Integer vector of action indices.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]: Tuple of
            next states, rewards, terminal flags, and debugging dictionaries.
        &#34;&#34;&#34;
        raise NotImplementedError

    @abstractmethod
    def reset_bulk(self, n: int) -&gt; np.ndarray:
        &#34;&#34;&#34;Provides multiple new environment states.

        Args:
            n (int): Number of states to generate.

        Returns:
            np.ndarray: Initial states, stacked in the first dimension.
        &#34;&#34;&#34;
        raise NotImplementedError

    def reset(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Provides a single new environment state.

        Returns:
            np.ndarray: Initial state
        &#34;&#34;&#34;
        return self.reset_bulk(1)[0]

    @classmethod
    def get_factory(cls: T, *args, **kwargs) -&gt; &#34;ai.simulators.Factory[T]&#34;:
        &#34;&#34;&#34;Creates and returns a factory object that spawns simulators when called.

        Args and kwargs are passed along to the class constructor. However, if other
        behavior is required, feel free to override this method and return a factory
        class of your choice.&#34;&#34;&#34;
        return ai.simulators.Factory[T](cls, *args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>ai.simulators._connect_four.ConnectFour</li>
<li>ai.simulators._tictactoe.TicTacToe</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ai.simulators.Base.get_factory"><code class="name flex">
<span>def <span class="ident">get_factory</span></span>(<span>*args, **kwargs) â>Â ai.simulators._factory.Factory[~T]</span>
</code></dt>
<dd>
<div class="desc"><p>Creates and returns a factory object that spawns simulators when called.</p>
<p>Args and kwargs are passed along to the class constructor. However, if other
behavior is required, feel free to override this method and return a factory
class of your choice.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_factory(cls: T, *args, **kwargs) -&gt; &#34;ai.simulators.Factory[T]&#34;:
    &#34;&#34;&#34;Creates and returns a factory object that spawns simulators when called.

    Args and kwargs are passed along to the class constructor. However, if other
    behavior is required, feel free to override this method and return a factory
    class of your choice.&#34;&#34;&#34;
    return ai.simulators.Factory[T](cls, *args, **kwargs)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="ai.simulators.Base.action_space"><code class="name">var <span class="ident">action_space</span> :Â ai.simulators.action_spaces._base.Base</code></dt>
<dd>
<div class="desc"><p>The action space class used by this simulator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@abstractmethod
def action_space(self) -&gt; action_spaces.Base:
    &#34;&#34;&#34;The action space class used by this simulator.&#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ai.simulators.Base.deterministic"><code class="name">var <span class="ident">deterministic</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def deterministic(self) -&gt; bool:
    return self._deterministic</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ai.simulators.Base.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Provides a single new environment state.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Initial state</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; np.ndarray:
    &#34;&#34;&#34;Provides a single new environment state.

    Returns:
        np.ndarray: Initial state
    &#34;&#34;&#34;
    return self.reset_bulk(1)[0]</code></pre>
</details>
</dd>
<dt id="ai.simulators.Base.reset_bulk"><code class="name flex">
<span>def <span class="ident">reset_bulk</span></span>(<span>self, n:Â int) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Provides multiple new environment states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of states to generate.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Initial states, stacked in the first dimension.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def reset_bulk(self, n: int) -&gt; np.ndarray:
    &#34;&#34;&#34;Provides multiple new environment states.

    Args:
        n (int): Number of states to generate.

    Returns:
        np.ndarray: Initial states, stacked in the first dimension.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ai.simulators.Base.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, state:Â numpy.ndarray, action:Â int) â>Â Tuple[numpy.ndarray,Â float,Â bool,Â Dict]</span>
</code></dt>
<dd>
<div class="desc"><p>Executes one step in the environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>State</dd>
<dt><strong><code>action</code></strong> :&ensp;<code>int</code></dt>
<dd>Action index</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, float, bool, Dict]</code></dt>
<dd>Tuple of next state, reward, terminal</dd>
</dl>
<p>flag, and debugging dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(
    self, state: np.ndarray, action: int
) -&gt; Tuple[np.ndarray, float, bool, Dict]:
    &#34;&#34;&#34;Executes one step in the environment.

    Args:
        state (np.ndarray): State
        action (int): Action index

    Returns:
        Tuple[np.ndarray, float, bool, Dict]: Tuple of next state, reward, terminal
        flag, and debugging dictionary.
    &#34;&#34;&#34;
    next_states, rewards, terminals, infos = self.step_bulk(
        np.expand_dims(state, 0), np.array([action])
    )
    return next_states[0], rewards[0], terminals[0], infos[0]</code></pre>
</details>
</dd>
<dt id="ai.simulators.Base.step_bulk"><code class="name flex">
<span>def <span class="ident">step_bulk</span></span>(<span>self, states:Â numpy.ndarray, actions:Â numpy.ndarray) â>Â Tuple[numpy.ndarray,Â numpy.ndarray,Â numpy.ndarray,Â List[Dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a bulk of actions in multiple states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>States, in batch format.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Integer vector of action indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]</code></dt>
<dd>Tuple of</dd>
</dl>
<p>next states, rewards, terminal flags, and debugging dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def step_bulk(
    self, states: np.ndarray, actions: np.ndarray
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:
    &#34;&#34;&#34;Executes a bulk of actions in multiple states.

    Args:
        states (np.ndarray): States, in batch format.
        actions (np.ndarray): Integer vector of action indices.

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]: Tuple of
        next states, rewards, terminal flags, and debugging dictionaries.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ai.simulators.ConnectFour"><code class="flex name class">
<span>class <span class="ident">ConnectFour</span></span>
</code></dt>
<dd>
<div class="desc"><p>Connect four (four in a row) game simulator.</p>
<p>States are given by a single <code>np.ndarray</code> of shape <code>(43, )</code>. The first 42 elements
denote the game board in row-major order (board is of shape <code>(6, 7)</code>). Each board
element is in <code>{-1, 0, 1}</code>, where <code>-1</code> and <code>1</code> denote occupied cells and <code>0</code> empty
cells. The last element in the state vector (i.e. element index 42) is either <code>+1</code>
or <code>-1</code>, denoting the player who is about to play.</p>
<p>Actions are discrete in <code>{0, 1, ..., 6}</code>, denoting, from the left, which column to
place the next marker in.</p>
<p>Rewards are given at the end of a game round, i.e. intermediate rewards are zero.
Then, if a winning action is rewarded with <code>+1</code> and a losing action is rewarded with
<code>-1</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>deterministic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag indicating if this simulator instance is
considered deterministic or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConnectFour(Base):
    &#34;&#34;&#34;Connect four (four in a row) game simulator.

    States are given by a single `np.ndarray` of shape `(43, )`. The first 42 elements
    denote the game board in row-major order (board is of shape `(6, 7)`). Each board
    element is in `{-1, 0, 1}`, where `-1` and `1` denote occupied cells and `0` empty
    cells. The last element in the state vector (i.e. element index 42) is either `+1`
    or `-1`, denoting the player who is about to play.

    Actions are discrete in `{0, 1, ..., 6}`, denoting, from the left, which column to
    place the next marker in.

    Rewards are given at the end of a game round, i.e. intermediate rewards are zero.
    Then, if a winning action is rewarded with `+1` and a losing action is rewarded with
    `-1`.&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        super().__init__(True)
        self._action_space = action_spaces.ConnectFour()

    def reset_bulk(self, n: int) -&gt; np.ndarray:
        states = np.zeros((n, 7 * 6 + 1))
        states[:, -1] = 1.0
        return states

    @property
    def action_space(self) -&gt; action_spaces.ConnectFour:
        return self._action_space

    def _compute_rewards(self, states: np.ndarray, players: np.ndarray) -&gt; np.ndarray:

        players = 4 * players.reshape((-1, 1, 1))

        win_horisontal = convolve(states, np.ones((1, 1, 4)), mode=&#34;valid&#34;) == players
        win_horisontal = np.any(np.any(win_horisontal, axis=2), axis=1)

        win_vertical = convolve(states, np.ones((1, 4, 1)), mode=&#34;valid&#34;) == players
        win_vertical = np.any(np.any(win_vertical, axis=2), axis=1)

        win_diagonal = (
            convolve(states, np.diag(np.ones(4)).reshape((1, 4, 4)), mode=&#34;valid&#34;)
            == players
        )
        win_diagonal = np.any(np.any(win_diagonal, axis=2), axis=1)

        win_xdiagonal = (
            convolve(states, cross_diag(np.ones(4)).reshape((1, 4, 4)), mode=&#34;valid&#34;)
            == players
        )
        win_xdiagonal = np.any(np.any(win_xdiagonal, axis=2), axis=1)

        win = win_horisontal | win_vertical | win_diagonal | win_xdiagonal

        loss_horisontal = convolve(states, np.ones((1, 1, 4)), mode=&#34;valid&#34;) == -players
        loss_horisontal = np.any(np.any(loss_horisontal, axis=2), axis=1)

        loss_vertical = convolve(states, np.ones((1, 4, 1)), mode=&#34;valid&#34;) == -players
        loss_vertical = np.any(np.any(loss_vertical, axis=2), axis=1)

        loss_diagonal = (
            convolve(states, np.diag(np.ones(4)).reshape((1, 4, 4)), mode=&#34;valid&#34;)
            == -players
        )
        loss_diagonal = np.any(np.any(loss_diagonal, axis=2), axis=1)

        loss_xdiagonal = (
            convolve(states, cross_diag(np.ones(4)).reshape((1, 4, 4)), mode=&#34;valid&#34;)
            == -players
        )
        loss_xdiagonal = np.any(np.any(loss_xdiagonal, axis=2), axis=1)

        loss = loss_horisontal | loss_vertical | loss_diagonal | loss_xdiagonal

        rewards = np.zeros(states.shape[0])
        rewards[win] = 1.0
        rewards[loss] = -1.0

        return rewards

    def step_bulk(
        self, states: np.ndarray, actions: np.ndarray
    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:

        next_states = states.copy()
        batchvec = np.arange(next_states.shape[0])
        players = next_states[:, -1]

        next_states = next_states[:, :-1].reshape((-1, 6, 7))
        heights = (next_states != 0).sum(axis=1)

        if np.any(heights[batchvec, actions] &gt;= 6):
            raise ValueError(&#34;Cannot place a piece in an already full column&#34;)

        next_states[batchvec, 5 - heights[batchvec, actions], actions] = players[
            batchvec
        ]
        heights[batchvec, actions] += 1
        rewards = self._compute_rewards(next_states, players)
        terminals = (rewards != 0) | np.all(heights == 6, axis=1)

        next_states = next_states.reshape((-1, 6 * 7))
        next_states = np.concatenate((next_states, -players.reshape((-1, 1))), axis=1)

        return (
            next_states,
            rewards,
            terminals,
            [{} for _ in range(next_states.shape[0])],
        )

    def render(self, state: np.ndarray, output_fn: Callable[[str], None] = print):
        &#34;&#34;&#34;Renders the game board to a string and then outputs it to the given
        output function.

        Args:
            state (np.ndarray): State to render
            output_fn (Callable[[str], None], optional): Output function, accepting one
            string argument. Defaults to `print`.
        &#34;&#34;&#34;

        def tile(value) -&gt; str:
            if value == 0:
                return &#34; &#34;
            elif value == 1:
                return &#34;X&#34;
            elif value == -1:
                return &#34;O&#34;
            else:
                raise ValueError(f&#34;Unexpected value {value}&#34;)

        def print_line(i: int):
            output_fn(&#34; | &#34;.join(tile(state[7 * i + j]) for j in range(7)))

        output_fn(&#34; | &#34;.join(str(x) for x in range(7)))
        for i in range(6):
            print_line(i)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>ai.simulators._base.Base</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="ai.simulators.ConnectFour.action_space"><code class="name">var <span class="ident">action_space</span> :Â ai.simulators.action_spaces._connect_four.ConnectFour</code></dt>
<dd>
<div class="desc"><p>The action space class used by this simulator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def action_space(self) -&gt; action_spaces.ConnectFour:
    return self._action_space</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ai.simulators.ConnectFour.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self, state:Â numpy.ndarray, output_fn:Â Callable[[str],Â NoneType]Â =Â &lt;built-in function print&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Renders the game board to a string and then outputs it to the given
output function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>State to render</dd>
<dt><strong><code>output_fn</code></strong> :&ensp;<code>Callable[[str], None]</code>, optional</dt>
<dd>Output function, accepting one</dd>
</dl>
<p>string argument. Defaults to <code>print</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render(self, state: np.ndarray, output_fn: Callable[[str], None] = print):
    &#34;&#34;&#34;Renders the game board to a string and then outputs it to the given
    output function.

    Args:
        state (np.ndarray): State to render
        output_fn (Callable[[str], None], optional): Output function, accepting one
        string argument. Defaults to `print`.
    &#34;&#34;&#34;

    def tile(value) -&gt; str:
        if value == 0:
            return &#34; &#34;
        elif value == 1:
            return &#34;X&#34;
        elif value == -1:
            return &#34;O&#34;
        else:
            raise ValueError(f&#34;Unexpected value {value}&#34;)

    def print_line(i: int):
        output_fn(&#34; | &#34;.join(tile(state[7 * i + j]) for j in range(7)))

    output_fn(&#34; | &#34;.join(str(x) for x in range(7)))
    for i in range(6):
        print_line(i)</code></pre>
</details>
</dd>
<dt id="ai.simulators.ConnectFour.reset_bulk"><code class="name flex">
<span>def <span class="ident">reset_bulk</span></span>(<span>self, n:Â int) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Provides multiple new environment states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of states to generate.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Initial states, stacked in the first dimension.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_bulk(self, n: int) -&gt; np.ndarray:
    states = np.zeros((n, 7 * 6 + 1))
    states[:, -1] = 1.0
    return states</code></pre>
</details>
</dd>
<dt id="ai.simulators.ConnectFour.step_bulk"><code class="name flex">
<span>def <span class="ident">step_bulk</span></span>(<span>self, states:Â numpy.ndarray, actions:Â numpy.ndarray) â>Â Tuple[numpy.ndarray,Â numpy.ndarray,Â numpy.ndarray,Â List[Dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a bulk of actions in multiple states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>States, in batch format.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Integer vector of action indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]</code></dt>
<dd>Tuple of</dd>
</dl>
<p>next states, rewards, terminal flags, and debugging dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_bulk(
    self, states: np.ndarray, actions: np.ndarray
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:

    next_states = states.copy()
    batchvec = np.arange(next_states.shape[0])
    players = next_states[:, -1]

    next_states = next_states[:, :-1].reshape((-1, 6, 7))
    heights = (next_states != 0).sum(axis=1)

    if np.any(heights[batchvec, actions] &gt;= 6):
        raise ValueError(&#34;Cannot place a piece in an already full column&#34;)

    next_states[batchvec, 5 - heights[batchvec, actions], actions] = players[
        batchvec
    ]
    heights[batchvec, actions] += 1
    rewards = self._compute_rewards(next_states, players)
    terminals = (rewards != 0) | np.all(heights == 6, axis=1)

    next_states = next_states.reshape((-1, 6 * 7))
    next_states = np.concatenate((next_states, -players.reshape((-1, 1))), axis=1)

    return (
        next_states,
        rewards,
        terminals,
        [{} for _ in range(next_states.shape[0])],
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ai.simulators.Factory"><code class="flex name class">
<span>class <span class="ident">Factory</span></span>
<span>(</span><span>cls:Â ~T, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Factories are callable objects that spawn simulator instances.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Factory(Generic[T]):
    &#34;&#34;&#34;Factories are callable objects that spawn simulator instances.&#34;&#34;&#34;

    def __init__(self, cls: T, *args, **kwargs):
        super().__init__()
        self._cls = cls
        self._args = args
        self._kwargs = kwargs

    def __call__(self) -&gt; T:
        return self._cls(*self._args, **self._kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
</dd>
<dt id="ai.simulators.TicTacToe"><code class="flex name class">
<span>class <span class="ident">TicTacToe</span></span>
</code></dt>
<dd>
<div class="desc"><p>TicTacToe (connect three, or three in a row) simulator.</p>
<p>States are given by a single <code>np.ndarray</code> of shape <code>(10, )</code>. The first 9 elements
denote the game board in row-major order (board is of shape <code>(3, 3)</code>). Each board
element is in <code>{-1, 0, 1}</code>, where <code>-1</code> and <code>1</code> denote occupied cells and <code>0</code> empty
cells. The last element in the state vector (i.e. element index 9) is either <code>+1</code>
or <code>-1</code>, denoting the player who is about to play.</p>
<p>Actions discrete in <code>{0, 1, ..., 8}</code>, denoting, in row-major order, which cell to
place the next marker in.</p>
<p>Rewards are given at the end of a game round, i.e. intermediate rewards are zero.
Then, if a winning action is rewarded with <code>+1</code> and a losing action is rewarded with
<code>-1</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>deterministic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag indicating if this simulator instance is
considered deterministic or not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TicTacToe(Base):

    &#34;&#34;&#34;TicTacToe (connect three, or three in a row) simulator.

    States are given by a single `np.ndarray` of shape `(10, )`. The first 9 elements
    denote the game board in row-major order (board is of shape `(3, 3)`). Each board
    element is in `{-1, 0, 1}`, where `-1` and `1` denote occupied cells and `0` empty
    cells. The last element in the state vector (i.e. element index 9) is either `+1`
    or `-1`, denoting the player who is about to play.

    Actions discrete in `{0, 1, ..., 8}`, denoting, in row-major order, which cell to
    place the next marker in.

    Rewards are given at the end of a game round, i.e. intermediate rewards are zero.
    Then, if a winning action is rewarded with `+1` and a losing action is rewarded with
    `-1`.&#34;&#34;&#34;

    def __init__(self) -&gt; None:
        super().__init__(True)
        self._action_space = action_spaces.TicTacToe()

    @property
    def action_space(self) -&gt; action_spaces.TicTacToe:
        return self._action_space

    def reset_bulk(self, n: int) -&gt; np.ndarray:
        states = np.zeros((n, 10))
        states[:, -1] = 1.0
        return states

    def _check_win(self, states: np.ndarray) -&gt; np.ndarray:
        batchvec = np.arange(states.shape[0])
        repeated_batchvec = np.repeat(batchvec, 3)
        tiled_diag_indices = np.tile(_DIAG_INDICES, batchvec.shape[0])
        tiled_cross_diag_indices = np.tile(_CROSS_DIAG_INDICES, batchvec.shape[0])

        player = np.reshape(states[batchvec, -1], (-1, 1))
        own_marks = states[:, :-1] == player

        row_win = np.any(np.all(np.reshape(own_marks, (-1, 3, 3)), axis=2), axis=1)
        col_win = np.any(np.all(np.reshape(own_marks, (-1, 3, 3)), axis=1), axis=1)
        diagwin = np.all(
            np.reshape(own_marks[repeated_batchvec, tiled_diag_indices], (-1, 3)),
            axis=1,
        )
        crossdiagwin = np.all(
            np.reshape(own_marks[repeated_batchvec, tiled_cross_diag_indices], (-1, 3)),
            axis=1,
        )

        return row_win | col_win | diagwin | crossdiagwin

    def _check_loss(self, states: np.ndarray) -&gt; np.ndarray:
        states = states.copy()
        states[:, -1] = -states[:, -1]
        return self._check_win(states)

    def step_bulk(
        self, states: np.ndarray, actions: np.ndarray
    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:
        states = states

        if np.any(self._check_loss(states)) or np.any(self._check_win(states)):
            raise ValueError(&#34;Cannot step from a state already in a win condition.&#34;)

        next_states = states.copy()
        batchvec = np.arange(next_states.shape[0])

        if np.any(next_states[batchvec, actions] != 0.0):
            raise ValueError(&#34;Cannot place a piece at an already occupied spot&#34;)

        next_states[batchvec, actions] = next_states[batchvec, -1]

        rewards = np.zeros(next_states.shape[0])
        win = self._check_win(next_states)
        rewards[win] = 1.0
        loss = self._check_loss(next_states)
        rewards[loss] = -1.0

        terminals = win | loss | np.all(next_states != 0, axis=1)

        next_states[batchvec, -1] = -states[batchvec, -1]

        return (
            next_states,
            rewards,
            terminals,
            [{} for _ in range(next_states.shape[0])],
        )

    def render(self, state: np.ndarray, output_fn: Callable[[str], None] = print):
        &#34;&#34;&#34;Renders the game board and action index map to a string that is then output
        through the given output function.

        Args:
            state (np.ndarray): State to render
            output_fn (Callable[[str], None], optional): Output function, called with
            the generated string. Defaults to `print`.
        &#34;&#34;&#34;

        def tile(value) -&gt; str:
            if value == 0:
                return &#34; &#34;
            elif value == 1:
                return &#34;X&#34;
            elif value == -1:
                return &#34;O&#34;
            else:
                raise ValueError(f&#34;Unexpected value {value}&#34;)

        output_fn(
            f&#34;&#34;&#34;
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[0])}  |  {tile(state[1])}  |  {tile(state[2])}  |     |  0  |  1  |  2  |
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[3])}  |  {tile(state[4])}  |  {tile(state[5])}  |     |  3  |  4  |  5  |
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[6])}  |  {tile(state[7])}  |  {tile(state[8])}  |     |  6  |  7  |  8  |
| --- | --- | --- |         | --- | --- | --- |
        &#34;&#34;&#34;
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>ai.simulators._base.Base</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="ai.simulators.TicTacToe.action_space"><code class="name">var <span class="ident">action_space</span> :Â ai.simulators.action_spaces._tic_tac_toe.TicTacToe</code></dt>
<dd>
<div class="desc"><p>The action space class used by this simulator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def action_space(self) -&gt; action_spaces.TicTacToe:
    return self._action_space</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ai.simulators.TicTacToe.render"><code class="name flex">
<span>def <span class="ident">render</span></span>(<span>self, state:Â numpy.ndarray, output_fn:Â Callable[[str],Â NoneType]Â =Â &lt;built-in function print&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Renders the game board and action index map to a string that is then output
through the given output function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>State to render</dd>
<dt><strong><code>output_fn</code></strong> :&ensp;<code>Callable[[str], None]</code>, optional</dt>
<dd>Output function, called with</dd>
</dl>
<p>the generated string. Defaults to <code>print</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def render(self, state: np.ndarray, output_fn: Callable[[str], None] = print):
        &#34;&#34;&#34;Renders the game board and action index map to a string that is then output
        through the given output function.

        Args:
            state (np.ndarray): State to render
            output_fn (Callable[[str], None], optional): Output function, called with
            the generated string. Defaults to `print`.
        &#34;&#34;&#34;

        def tile(value) -&gt; str:
            if value == 0:
                return &#34; &#34;
            elif value == 1:
                return &#34;X&#34;
            elif value == -1:
                return &#34;O&#34;
            else:
                raise ValueError(f&#34;Unexpected value {value}&#34;)

        output_fn(
            f&#34;&#34;&#34;
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[0])}  |  {tile(state[1])}  |  {tile(state[2])}  |     |  0  |  1  |  2  |
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[3])}  |  {tile(state[4])}  |  {tile(state[5])}  |     |  3  |  4  |  5  |
| --- | --- | --- |         | --- | --- | --- |
|  {tile(state[6])}  |  {tile(state[7])}  |  {tile(state[8])}  |     |  6  |  7  |  8  |
| --- | --- | --- |         | --- | --- | --- |
        &#34;&#34;&#34;
        )</code></pre>
</details>
</dd>
<dt id="ai.simulators.TicTacToe.reset_bulk"><code class="name flex">
<span>def <span class="ident">reset_bulk</span></span>(<span>self, n:Â int) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Provides multiple new environment states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of states to generate.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Initial states, stacked in the first dimension.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_bulk(self, n: int) -&gt; np.ndarray:
    states = np.zeros((n, 10))
    states[:, -1] = 1.0
    return states</code></pre>
</details>
</dd>
<dt id="ai.simulators.TicTacToe.step_bulk"><code class="name flex">
<span>def <span class="ident">step_bulk</span></span>(<span>self, states:Â numpy.ndarray, actions:Â numpy.ndarray) â>Â Tuple[numpy.ndarray,Â numpy.ndarray,Â numpy.ndarray,Â List[Dict]]</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a bulk of actions in multiple states.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>States, in batch format.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Integer vector of action indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]</code></dt>
<dd>Tuple of</dd>
</dl>
<p>next states, rewards, terminal flags, and debugging dictionaries.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_bulk(
    self, states: np.ndarray, actions: np.ndarray
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, List[Dict]]:
    states = states

    if np.any(self._check_loss(states)) or np.any(self._check_win(states)):
        raise ValueError(&#34;Cannot step from a state already in a win condition.&#34;)

    next_states = states.copy()
    batchvec = np.arange(next_states.shape[0])

    if np.any(next_states[batchvec, actions] != 0.0):
        raise ValueError(&#34;Cannot place a piece at an already occupied spot&#34;)

    next_states[batchvec, actions] = next_states[batchvec, -1]

    rewards = np.zeros(next_states.shape[0])
    win = self._check_win(next_states)
    rewards[win] = 1.0
    loss = self._check_loss(next_states)
    rewards[loss] = -1.0

    terminals = win | loss | np.all(next_states != 0, axis=1)

    next_states[batchvec, -1] = -states[batchvec, -1]

    return (
        next_states,
        rewards,
        terminals,
        [{} for _ in range(next_states.shape[0])],
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ai" href="../index.html">ai</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ai.simulators.action_spaces" href="action_spaces/index.html">ai.simulators.action_spaces</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ai.simulators.Base" href="#ai.simulators.Base">Base</a></code></h4>
<ul class="two-column">
<li><code><a title="ai.simulators.Base.action_space" href="#ai.simulators.Base.action_space">action_space</a></code></li>
<li><code><a title="ai.simulators.Base.deterministic" href="#ai.simulators.Base.deterministic">deterministic</a></code></li>
<li><code><a title="ai.simulators.Base.get_factory" href="#ai.simulators.Base.get_factory">get_factory</a></code></li>
<li><code><a title="ai.simulators.Base.reset" href="#ai.simulators.Base.reset">reset</a></code></li>
<li><code><a title="ai.simulators.Base.reset_bulk" href="#ai.simulators.Base.reset_bulk">reset_bulk</a></code></li>
<li><code><a title="ai.simulators.Base.step" href="#ai.simulators.Base.step">step</a></code></li>
<li><code><a title="ai.simulators.Base.step_bulk" href="#ai.simulators.Base.step_bulk">step_bulk</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ai.simulators.ConnectFour" href="#ai.simulators.ConnectFour">ConnectFour</a></code></h4>
<ul class="">
<li><code><a title="ai.simulators.ConnectFour.action_space" href="#ai.simulators.ConnectFour.action_space">action_space</a></code></li>
<li><code><a title="ai.simulators.ConnectFour.render" href="#ai.simulators.ConnectFour.render">render</a></code></li>
<li><code><a title="ai.simulators.ConnectFour.reset_bulk" href="#ai.simulators.ConnectFour.reset_bulk">reset_bulk</a></code></li>
<li><code><a title="ai.simulators.ConnectFour.step_bulk" href="#ai.simulators.ConnectFour.step_bulk">step_bulk</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ai.simulators.Factory" href="#ai.simulators.Factory">Factory</a></code></h4>
</li>
<li>
<h4><code><a title="ai.simulators.TicTacToe" href="#ai.simulators.TicTacToe">TicTacToe</a></code></h4>
<ul class="">
<li><code><a title="ai.simulators.TicTacToe.action_space" href="#ai.simulators.TicTacToe.action_space">action_space</a></code></li>
<li><code><a title="ai.simulators.TicTacToe.render" href="#ai.simulators.TicTacToe.render">render</a></code></li>
<li><code><a title="ai.simulators.TicTacToe.reset_bulk" href="#ai.simulators.TicTacToe.reset_bulk">reset_bulk</a></code></li>
<li><code><a title="ai.simulators.TicTacToe.step_bulk" href="#ai.simulators.TicTacToe.step_bulk">step_bulk</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>